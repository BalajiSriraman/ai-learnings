{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (4.3.3)\n",
      "Requirement already satisfied: nltk in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (1.6.1)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp310-cp310-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: joblib in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: click in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: tqdm in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/balaji/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/balaji/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Download tokenizer and dataset\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind', '-', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'didn', \"'\", 't', 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'\", 's', 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'\", 's', 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'don', \"'\", 't', 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'\", 's', 'biggest', 'problem', '.', 'it', \"'\", 's', 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half', '-', 'way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'didn', \"'\", 't', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'don', \"'\", 't', 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'\", 've', 'been', 'a', 'pretty', 'decent', 'teen', 'mind', '-', 'fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'\", 's', 'unraveling', '.', 'overall', ',', 'the', 'film', 'doesn', \"'\", 't', 'stick', 'because', 'it', 'doesn', \"'\", 't', 'entertain', ',', 'it', \"'\", 's', 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'\", 's', 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'\", 's', 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7', '/', '10', ')', '-', 'blair', 'witch', '2', '(', '7', '/', '10', ')', '-', 'the', 'crow', '(', '9', '/', '10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4', '/', '10', ')', '-', 'lost', 'highway', '(', '10', '/', '10', ')', '-', 'memento', '(', '10', '/', '10', ')', '-', 'the', 'others', '(', '9', '/', '10', ')', '-', 'stir', 'of', 'echoes', '(', '8', '/', '10', ')']\n",
      "neg\n",
      "[-0.14773785  0.1301635   0.08359744 -0.03098643 -0.09758037 -0.39486343\n",
      "  0.19006075  0.58005637 -0.18752618 -0.18077812  0.07049639 -0.3294734\n",
      "  0.14886802  0.2514866   0.09349232  0.01591366  0.1768397  -0.24041831\n",
      "  0.03144111 -0.37153053  0.12488888 -0.0817749   0.26382625 -0.22682916\n",
      " -0.0545507   0.03450905 -0.3755402   0.06936175 -0.34566656  0.20880723\n",
      "  0.3274847  -0.1240048   0.16455439 -0.47247225 -0.17407428  0.23582922\n",
      "  0.268778   -0.04759115 -0.27642813 -0.15123941  0.11171352 -0.04881766\n",
      " -0.2701702  -0.03359301  0.06336658 -0.06012855 -0.05631808 -0.12894665\n",
      "  0.07238503  0.079426    0.0365845  -0.26722252 -0.09146132 -0.24811682\n",
      "  0.00226743 -0.03061954  0.04170674  0.0136344  -0.08950702  0.2750493\n",
      " -0.17042784 -0.13006707  0.21679531 -0.01035393 -0.06962314  0.2963046\n",
      "  0.05706866  0.19474275 -0.22831687  0.22254965 -0.00398866  0.19285397\n",
      "  0.33895206  0.01557332  0.175871   -0.05893867  0.1592966  -0.02296592\n",
      " -0.21615866 -0.05256431 -0.29653105 -0.15453734 -0.17477986  0.318332\n",
      " -0.1598919  -0.17987168  0.10166954  0.0675874   0.20757398  0.11640999\n",
      "  0.2912311   0.10157031 -0.04587048 -0.05796181  0.30627355  0.19878247\n",
      "  0.22406207 -0.0737565   0.18974361 -0.24850024]\n",
      "Classifier Accuracy: 0.75\n",
      "Sentence: The movie was fantastic and had great visuals -> Sentiment: Negative\n",
      "Sentence: I hated the plot and the acting was terrible -> Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Step 1: Load IMDb-like dataset (from NLTK movie reviews)\n",
    "data = [(list(movie_reviews.words(fileid)), category)\n",
    "        for category in movie_reviews.categories()\n",
    "        for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Split data into sentences (reviews) and labels\n",
    "sentences, labels = zip(*data)\n",
    "print(sentences[0])\n",
    "print(labels[0])\n",
    "# Convert labels into binary format\n",
    "labels = [1 if label == \"pos\" else 0 for label in labels]\n",
    "\n",
    "# Tokenize the dataset (already tokenized in NLTK; skip this if necessary)\n",
    "tokenized_sentences = [sentence for sentence in sentences]\n",
    "\n",
    "# Step 2: Train Word2Vec model on the tokenized sentences\n",
    "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Function to compute sentence embeddings by averaging word vectors\n",
    "def get_sentence_embedding(sentence_tokens, model):\n",
    "    word_vectors = [model.wv[word] for word in sentence_tokens if word in model.wv]\n",
    "    if len(word_vectors) > 0:\n",
    "        return np.mean(word_vectors, axis=0)  # Average word vectors\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return a zero vector if no words are in the model\n",
    "\n",
    "# Step 3: Compute embeddings for all sentences\n",
    "X = np.array([get_sentence_embedding(tokens, word2vec_model) for tokens in tokenized_sentences])\n",
    "y = np.array(labels)\n",
    "print(X[0])\n",
    "# Step 4: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train a classifier (Logistic Regression)\n",
    "classifier = make_pipeline(StandardScaler(), LogisticRegression(C=1))\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Test the classifier\n",
    "accuracy = classifier.score(X_test, y_test)\n",
    "print(f\"Classifier Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Step 7: Predict on new sentences\n",
    "new_sentences = [\"The movie was fantastic and had great visuals\",\n",
    "                 \"I hated the plot and the acting was terrible\"]\n",
    "new_tokenized = [word_tokenize(sentence.lower()) for sentence in new_sentences]\n",
    "new_embeddings = np.array([get_sentence_embedding(tokens, word2vec_model) for tokens in new_tokenized])\n",
    "\n",
    "# Predict using the trained classifier\n",
    "predictions = classifier.predict(new_embeddings)\n",
    "for sentence, prediction in zip(new_sentences, predictions):\n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    print(f\"Sentence: {sentence} -> Sentiment: {sentiment}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
